```yaml
category: machine learning
entry: glossary
```

实例(instance), 样本(sample):　关于一个事件或对象的描述
数据集(data set): 所有样本的集合，包括训练集和测试集
属性(attributes) 特征(feature): 反映事件或对象再某方面的表现或性质的事项
属性空间, 样本空间, 输入空间: 属性张成的空间
特征向量: 每个样本都可对应到的样本空间上的一个点 每个点对应的一个座标向量
训练, 学习: 从数据中学得模型的过程, 通过执行某个学习算法来完成
训练数据: 训练过程中使用到数据
假设: 学得模型对应了关于数据的某种潜在的规律
真相: 潜在规律的自身　学习过程就是为了找出或者逼近规律
学习器: 学得的模型
标记: 拥有标记信息的示例
标记空间, 输出空间: 标记的集合
分类 回归(监督学习)
测试: 学得模型之后,使用其进行预测的过程; 测试样本
聚类: 将数据分为若干组, 有助于了解数据内在规律，为更深入地分析数据建立基础, 学习过程中使用的样本没有标记信息（无监督学习）

归纳与演绎是科学推理的两大基本手段, 前者是从特殊到到一般的“泛化”的过程, 即从基础原理推演出具体状况

学习过程看作一个在所有假设(bypothesis)组成的空间中进行搜索的过程, 搜索目标是找到与训练集"匹配"(fit) 的假设, 即能够将训练集中的瓜判断正确的假设.

版本空间:

错误率(error rate): 分类错误的样本占样本总数的比例
精度(accuracy): 精度 = 1 - 错误率

学习器在训练集上的误差: 训练误差 经验误差
学习器在新样本上的误差: 泛化误差

过拟合(overfitting): 对训练样本学习的太好了, 把训练样本中的一些特点当作了所有潜在样本都会有的一般性质, 导致泛化性能下降
过拟合无法彻底避免, 机器学习面临的问题通常是 NP 难甚至更难问题
欠拟合(underfitting): 学习能力低下
比较容易克服. 例如在决策树学习中扩展分支, 在神经网络学习中增加训练轮数.

评估方法:

1. 留出法 Hold-out

将数据集 $D$ 划分成两个互斥的集合, 其中一个集合作为训练集 $S$, 另一个作为测试集 $T$, 即 $D=S\cup T,S\cap T=\varnothing$. 在 $S$ 上训练出模型后, 用 $T$ 评估其测试误差.

训练/测试集的划分要尽量保持数据分布的一致性. 从采样的角度来看待数据集划分的过程, 则保留类别比例的采样方法通常称为 "分层采样".

2. 交叉验证法 Cross Validation

先将数据集 $D$ 划分成 $k$ 个大小相似的互斥子集, 即 $D=D_1\cup D_2\cup D_3\dots\cup D_k,D_i\cap D_j=\varnothing(i\ne j)$. 每个子集 $D_i$ 都尽可能保持数据分布的一致性, 即从 $D$ 中通过分层采样得到. 然后每次用 $k-1$ 个子集的并集作为训练集, 余下的那个子集作为测试集. 这样可以获得 $k$ 组训练/测试集, 从而进行 $k$ 次训练和测试, 最终返回的是这 $k$ 个测试结果的均值.

交叉验证法的稳定性和保真性很大程度上取决于 $k$ 的取值, 通常把交叉验证法称为 "k 折交叉验证" (k-fold cross validation). 为了减小因样本划分不同而引入的差别, 通常要随机使用不同的划分重复 $p$ 次, 最终评估的结果是这 $p$ 次 $k$ 折交叉验证结果的均值.

3. 留一法 Leave-one-out

交叉验证法的一个特例. 将数据集划分成每个子集只包含一个样本的情况. 这使得在绝大多数情况下, 留一法中被实际评估的模型与期望评估的模型用 $D$ 训练出的模型很相似. 但是在数据集比较大时, 计算的开销很大.

4. 自助法 Bootstrapping

自助法以自助采样为基础, 给定包含 $m$ 个样本的数据集 $D$, 我们对他进行采样产生数据集 $D'$: 每次随机从 $D$ 选择一个样本拷贝到 $D'$ 中. 这个过程重复执行 $m$ 次后, 得到而包含 $m$ 个样本的数据集 $D'$. 显然, $D$ 中有部分样本在 $D'$ 多次出现, 另一部分不出现. 样本在 $m$ 次采样中始终不被采到的概率是

$$
\lim_{m\to\infty}\left( 1-\frac{1}{m} \right)^m\to \frac{1}{e}\approx 0.368
$$

自助法中实际评估的模型于期望评估的模型都使用 $m$ 个训练样本, 这样的测试结果亦称 "包外估计" (out-of-bag estimate)

自助法在数据集较小, 难以有效划分训练/测试集时很有用. 自助法能从数据集产生多个不同的训练集, 这对集成学习等方法很有用.

性能度量

衡量模型泛化性能能力的评价标准

给定样例集 $D=\left\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\right\}$

回归任务中常用的性能度量是 "均方误差" (mean squared error)

$$
E(f;D)=\frac{1}{m}\sum\limits_{i=1}^m \left( f(x_i)-y_i \right)^2
$$

更一般的, 对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot)$, 均方误差为:

$$
E(f;\mathcal{D})=\int\limits_{x\sim\mathcal{D}}\left( f(x)-y \right)^2p(x)dx
$$

错误率与精度

分类任务中最常用的两种性能度量, 既适用于二分类任务, 也适用于多分类任务.

错误率是分类错误的样本数占样本总数的比例

$$
E(f;D)=\frac{1}{m}\sum\limits_{i=1}^m \left( f(x_i)\ne y_i \right)
$$

精度是分类正确的样本占样本总数的比例

$$
\begin{aligned}
\text{acc}(f;D) & =\frac{1}{m}\sum\limits_{i=1}^m \left( f(x_i) = y_i \right)\\
& = 1 - E(f;D)
\end{aligned}
$$

查全率, 查准率, F1

对于二分类问题, 可以将真实类别与机器学习预测类别的组合划分成为 真正例 (true postitive), 假正例 (false postitive), 真反例 (true negative), 假反例 (false negative) 四种情况. 分别使用 TP, FP, TN, FN 表示. 显然, TP + FP + TN + FN = 样例总数.

查准率 P 与查全率 R 分别定义为

$$
P=\frac{TP}{TP+FP}\\
R=\frac{TP}{TP+FN}
$$

查准率和查全率是一对矛盾的度量. 一般来说, 查准率高时. 查全率往往偏低.

将样本按照预测结果中认为是 "正例" 的排序. 按此顺序进行预测, 就可以获得以查准率为纵轴, 查全率为横轴的曲线. 即 "查准率-查全率曲线" 简称 "P-R图"

在比较时, 一般认为如果一个学习器的曲线被另外一个学习器的曲线完全 "包住", 则可断言后者的性能优于前者.

如果两个学习器的曲线发生了交叉, 则需要使用别的标准来进行衡量比较.

平衡点 (Break-even point)

平衡点是 "查准率=查全率" 时的取值.

F1 度量

$$
F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{m+TP-TN}
$$

> F1 度量是基于查全率和查准率的调和平均数:
> $$
> \frac{1}{F1}=\frac{1}{2}\cdot \left( \frac{1}{P}+\frac{1}{R} \right)
> $$

F1 度量的一般形式 $F_{\beta}$ 能让我们表达出对 查准率/查全率 的不同偏好, 他的定义为

$$
F_{\beta}=\frac{(1+\beta)^2\times P\times R}{(\beta^2\times P)+R}
$$

> $F_{\beta}$ 度量是基于查全率和查准率的加权调和平均数
> $$
> \frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}\cdot \left( \frac{1}{P} + \frac{\beta^2}{R} \right)
> $$

其中 $\beta>0$, 度量了查全率对查准率的相对重要性. $\beta=1$ 时, 退化成标准的 F1 度量; $\beta>1$ 时, 查全率有更大的影响; $\beta<1$ 时, 查准率有更大的影响.

如果在进行多次二分类任务或者执行了多分类任务之后, 可以获得多个混淆矩阵, 一种直接的, 在 $n$ 个二分类混淆矩阵上综合考察查准率和查全率的办法就是分别计算之后查准率和查全率之后, 再计算平均值. 这就是 "宏查准率" (macro-P), "宏查全率" (macro-R), 以及相应的 "宏 F1" (macro-F1).

还可以先将混淆矩阵的对应元素进行平均, 得到 TP, FP, TN, FN 的平均值, 分别记为 $\overline{TP},\overline{FP},\overline{TN},\overline{FN}$, 再基于这些平均值求得 "微查准率" (micro-P), "微查全率" (micro-R) 和 "微 F1" (micro-F1).

ROC (Receiver Operationg Characteristic) 与 AUC

和 P-R 曲线类似, 将样本按照预测结果排序, 但是 ROC 的横轴是 "假正例率" (False Postitive Ratio), 纵轴是 "真正例率" (True Postitive Ratio) 两者的定义分别为:

$$
TPR=\frac{TP}{TP+FN}\\
FPR=\frac{FP}{TN+FP}
$$

进行学习器的比较时, 和 P-R 曲线一样, 若一个学习器的 ROC 曲线被另外一个学习的 ROC 曲线完全 "包住", 则可以认为后者的性能优于前者. 若两个学习器的 ROC 曲线发生交叉, 较为合理的平局是比较 ROC 曲线下的面积, 即 AUC (Area Under ROC Courve).

从定义可知, AUC 可通过对 ROC 曲线下的各部分的面积求积分得到. 假定 ROC 曲线是由坐标为 $\left\{ (x_1,y_1),(x_2,y_2),\dots,(x_m,y_m) \right\}$ 的点按序连接而成, 则 AUC 可估算为:

$$
AUC=\frac{1}{2}\sum\limits_{i=1}^{m=1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})
$$
